{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <a href=\"https://www.hi-paris.fr/\">\n",
    "        <img border=\"0\" src=\"https://www.hi-paris.fr/wp-content/uploads/2020/09/logo-hi-paris-retina.png\" width=\"25%\"></a>\n",
    "    <a href=\"https://www.dataia.eu/\">\n",
    "        <img border=\"0\" src=\"https://github.com/ramp-kits/template-kit/raw/main/img/DATAIA-h.png\" width=\"70%\"></a>\n",
    "</div>\n",
    "\n",
    "# DPE Label Prediction Challenge — Starting Kit\n",
    "\n",
    "<i> Datacamp 2025 — Codabench Challenge </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Context\n",
    "\n",
    "In France, every building put up for sale or rent must undergo a **Diagnostic de Performance Énergétique (DPE)** — an energy performance assessment that assigns a label from **A** (most efficient) to **G** (least efficient). This label directly impacts property value and informs public policy on building renovation ([see official documentation](https://www.service-public.fr/particuliers/vosdroits/F16096)).\n",
    "\n",
    "### The data\n",
    "\n",
    "The dataset used in this challenge comes from officially published DPE diagnostics for **existing housing** in France (`dpe03existant_2025.csv`). The raw dataset contains a large number of columns, many of which have significant missing values.\n",
    "\n",
    "#### Data selection process\n",
    "\n",
    "The following preprocessing steps were applied to produce the challenge dataset:\n",
    "\n",
    "1. **Column filtering** — All columns with **any missing values** were removed, ensuring a clean dataset with no imputation needed.\n",
    "2. **Geographic filtering** — Overseas territories (departments 971, 972, 973, 974, 988) were excluded. Only **metropolitan France** is kept.\n",
    "3. **Building type** — The data contains only **apartments** (`type_batiment = \"appartement\"`), as they represent a larger diversity of buildings and are present throughout the country.\n",
    "\n",
    "#### Features\n",
    "\n",
    "After filtering, the dataset contains a mix of:\n",
    "\n",
    "| Type | Examples |\n",
    "|------|----------|\n",
    "| **Categorical** | `type_installation_chauffage` (heating system type), `code_region_ban` (region code), `code_departement_ban` (department code) |\n",
    "| **Numerical** | Energy consumption metrics, surface area, and other physical characteristics of the dwelling |\n",
    "\n",
    "Key properties of the data:\n",
    "- **Outliers**: The numerical features exhibit significant outliers (visible in boxplots), requiring careful preprocessing decisions (normalization, clipping, exclusion, etc.).\n",
    "- **Correlations**: Some numerical features are correlated, opening the door to dimensionality reduction or feature selection strategies.\n",
    "- **Class imbalance**: The DPE labels (A–G) are not uniformly distributed — labels D and E dominate, while A and B are rare.\n",
    "\n",
    "#### Train / test split\n",
    "\n",
    "The data is split into **train (70%)** and **test (30%)** using a **stratified split** on both `etiquette_dpe` and `code_region_ban`, ensuring that the geographic and label distributions are preserved across splits. A further split separates the test set into a **public test** and a **private test** set.\n",
    "\n",
    "### The task\n",
    "\n",
    "**Given the characteristics of a dwelling, predict its DPE energy label (A–G).**\n",
    "\n",
    "This is a **multiclass classification** problem with 7 classes. Your model will be evaluated on its ability to correctly assign the right energy label to unseen dwellings.\n",
    "\n",
    "### Why does it matter?\n",
    "\n",
    "Accurate DPE prediction helps:\n",
    "- Identify buildings most in need of energy renovation\n",
    "- Support public policy on climate and housing\n",
    "- Enable real-estate professionals to estimate energy performance before a full diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "The goal of this section is to show what's in the data, and how to play with it.\n",
    "This is the first set in any data science project, and here, you should give a sense of the data the participants will be working with.\n",
    "\n",
    "You can first load and describe the data, and then show some interesting properties of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load the data\n",
    "from ingestion_program.ingestion import get_train_data\n",
    "X_df, y = get_train_data(\"dev_phase/input_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge evaluation\n",
    "\n",
    "A particularly important point in a challenge is to describe how it is evaluated. This is the section where you should describe the metric that will be used to evaluate the participants' submissions, as well as your evaluation strategy, in particular if there is some complexity in the way the data should be split to ensure valid results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission format\n",
    "\n",
    "Here, you should describe the submission format. This is the format the participants should follow to submit their predictions on the codabench platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The submission file\n",
    "\n",
    "The input data are stored in a dataframe. To go from a dataframe to a numpy array we will use a scikit-learn column transformer. The first example we will write will just consist in selecting a subset of columns we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/submission.py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# The submission here should simply be a function that returns a model\n",
    "# compatible with scikit-learn API\n",
    "def get_model():\n",
    "    return RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local testing pipeline\n",
    "\n",
    "Here you can show how the model will be used to generate predictions on the test set, and how the evaluation will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/.local/miniconda/lib/python3.12/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "X_train, y_train = get_train_data(\"dev_phase/input_data\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = pd.read_csv(\"dev_phase/input_data/test/test_features.csv\")\n",
    "from ingestion_program.ingestion import evaluate_model\n",
    "y_test = evaluate_model(model, X_test)\n",
    "\n",
    "from scoring_program.scoring import compute_accuracy\n",
    "print(\"Accuracy on test set:\", compute_accuracy(y_test, pd.read_csv(\"dev_phase/input_data/test/test_labels.csv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "To submit your code, you can refer to the actual challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
